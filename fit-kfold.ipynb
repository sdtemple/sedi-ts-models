{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6b25f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "params_file = None\n",
    "output_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57a239",
   "metadata": {},
   "source": [
    "# LSTM for sediment yield (K-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ba180-dfab-4eef-a668-6bf9802929ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch.functional import F\n",
    "import torch.nn as nn\n",
    "from ExlossUnivariate import ExlossUnivariate\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a525732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD PARAMETERS ===\n",
    "with open(params_file, 'r') as g:\n",
    "    f = json.load(g)\n",
    "    # data parameters\n",
    "    window_size = f['window_size']\n",
    "    window_step = f['window_step']\n",
    "    num_stations = f['num_stations']\n",
    "    X_scaler_path = f['X_scaler_path']\n",
    "    Y_scaler_path = f['Y_scaler_path']\n",
    "    # model parameters\n",
    "    lstm_hidden_size = f['lstm_hidden_size']\n",
    "    linear_hidden_size = f['linear_hidden_size']\n",
    "    lstm_num_layers = f['lstm_num_layers']\n",
    "    lstm_dropout= f['lstm_dropout']\n",
    "    linear_dropout= f['linear_dropout']\n",
    "    if_layer_norm = f['if_layer_norm']\n",
    "    if_gru = f['if_gru']\n",
    "    # training parameters\n",
    "    batch_size = f['batch_size']\n",
    "    num_epochs = f['num_epochs']\n",
    "    lr = f['learning_rate']\n",
    "    up_th = f['up_th']\n",
    "    down_th = f['down_th']\n",
    "    lambda_underestimate = f['lambda_underestimate']\n",
    "    lambda_overestimate = f['lambda_overestimate']\n",
    "    lambda_init = f['lambda_init']\n",
    "\n",
    "file_output = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = X_scaler_path\n",
    "X_loaded_scaler = joblib.load(scaler_filename)\n",
    "scaler_filename = Y_scaler_path\n",
    "Y_loaded_scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbe763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_func = np.exp\n",
    "def transform_func(x):\n",
    "    return 10 ** x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b790efc-98b2-4fec-b6f0-86efd8614b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Define Model ====\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 lstm_hidden_size,\n",
    "                 linear_hidden_size,\n",
    "                 lstm_num_layers=2,\n",
    "                 lstm_dropout=0.2,\n",
    "                 linear_dropout=0.5,\n",
    "                 if_layer_norm=True,\n",
    "                 if_gru=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if if_gru:\n",
    "            if lstm_num_layers > 1:\n",
    "                self.lstm = nn.GRU(input_size, \n",
    "                                    lstm_hidden_size, \n",
    "                                    lstm_num_layers, \n",
    "                                    batch_first=True,\n",
    "                                    dropout=lstm_dropout,\n",
    "                                )\n",
    "            else:\n",
    "                self.lstm = nn.GRU(input_size, \n",
    "                                    lstm_hidden_size, \n",
    "                                    1, \n",
    "                                    batch_first=True,\n",
    "                                    dropout=0.0,\n",
    "                                )\n",
    "        else:\n",
    "            if lstm_num_layers > 1:\n",
    "                self.lstm = nn.LSTM(input_size, \n",
    "                                    lstm_hidden_size, \n",
    "                                    lstm_num_layers, \n",
    "                                    batch_first=True,\n",
    "                                    dropout=lstm_dropout,\n",
    "                                )\n",
    "            else:\n",
    "                self.lstm = nn.LSTM(input_size, \n",
    "                                    lstm_hidden_size, \n",
    "                                    1, \n",
    "                                    batch_first=True,\n",
    "                                    dropout=0.0,\n",
    "                            )\n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden_size)\n",
    "        self.fc1 = nn.Linear(linear_hidden_size, linear_hidden_size)\n",
    "        self.dropout = nn.Dropout(p=linear_dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(linear_hidden_size, 1)\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.if_ = if_layer_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        # access dimension data\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        batch_size, sequence_length, num_features = x.shape\n",
    "        # all timesteps\n",
    "        out, _ = self.lstm(x)\n",
    "        # consider layer normalization\n",
    "        if self.if_:\n",
    "            out = self.layer_norm(out)\n",
    "        # flatten and linear layers\n",
    "        out = self.fc1(out.contiguous().view(-1, self.lstm_hidden_size))\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = out.view(batch_size, sequence_length, -1)\n",
    "        out = out.squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb1885-ae97-4113-83cc-9b6a5ce3f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kge(Y, Yhat):\n",
    "    '''\n",
    "    Compute Kling-Gupta efficiency\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : true values vector\n",
    "    Yhat : predicted values vector\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    float\n",
    "    '''\n",
    "    R = float(pearsonr(Y, Yhat).statistic[0])\n",
    "    Y_std = np.std(Y)\n",
    "    Yhat_std = np.std(Yhat)\n",
    "    Y_mu = np.mean(Y)\n",
    "    Yhat_mu = np.mean(Yhat)\n",
    "    beta = float(Yhat_mu / Y_mu)\n",
    "    alpha = float(Yhat_std / Y_std)\n",
    "    kappa = 1 - sqrt((R-1)**2 + (beta - 1)**2 + (alpha - 1)**2)\n",
    "    return kappa, beta, alpha, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ecea4f-f83d-4ee2-ac43-6d4c2bd1d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_weights(m):\n",
    "    if isinstance(m, torch.nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25816ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Data ====\n",
    "\n",
    "# Change to your target directory if needed, or specify the full path\n",
    "search_directory = \"training\" \n",
    "\n",
    "# Use '*' as a wildcard for the suffix\n",
    "pattern = os.path.join(search_directory, f\"X-size{window_size}-step{window_step}-station{num_stations}_*.npy\")\n",
    "\n",
    "# Find all files matching the pattern\n",
    "files_found = glob.glob(pattern)\n",
    "\n",
    "print(\"Files found using glob:\")\n",
    "years = []\n",
    "for file_path in files_found:\n",
    "    print(file_path)\n",
    "    years += [file_path[file_path.find('_')+1:-4].strip('_').split('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1320e70-bde0-47db-8cdf-63bdc4512be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found using glob:\n",
      "training/X-size60-step20-station50_2000_2004_2020.npy\n",
      "training/X-size60-step20-station50_1999_2009_2015.npy\n",
      "training/X-size60-step20-station50_1994_1998_2007.npy\n",
      "training/X-size60-step20-station50_1992_2002_2012.npy\n",
      "training/X-size60-step20-station50_2003_2005_2014.npy\n",
      "training/X-size60-step20-station50_1993_2008_2018.npy\n",
      "training/X-size60-step20-station50_1995_1997_2010.npy\n",
      "training/X-size60-step20-station50_2013_2017_2019.npy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(file_output,'w') as f:\n",
    "\n",
    "    f.write('Model parameters:\\n')\n",
    "    f.write('-----\\n')\n",
    "    f.write(f'lstm_hidden_size:{lstm_hidden_size}\\n')\n",
    "    f.write(f'linear_hidden_size:{linear_hidden_size}\\n')\n",
    "    f.write(f'lstm_num_layers:{lstm_num_layers}\\n')\n",
    "    f.write(f'lstm_dropout:{lstm_dropout}\\n')\n",
    "    f.write(f'linear_dropout:{linear_dropout}\\n')\n",
    "    f.write(f'if_layer_norm:{if_layer_norm}\\n')\n",
    "    f.write(f'if_gru:{if_gru}\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "    f.write('Training parameters:\\n')\n",
    "    f.write('-----\\n')\n",
    "    f.write(f'batch_size:{batch_size}\\n')\n",
    "    f.write(f'num_epochs:{num_epochs}\\n')\n",
    "    f.write(f'learning_rate:{lr}\\n')\n",
    "    f.write(f'up_th:{up_th}\\n') \n",
    "    f.write(f'down_th:{down_th}\\n')\n",
    "    f.write(f'lambda_underestimate:{lambda_underestimate}\\n')\n",
    "    f.write(f'lambda_overestimate:{lambda_overestimate}\\n')\n",
    "    f.write(f'lambda_init:{lambda_init}\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "    # data parameters\n",
    "    f.write('Data parameters:\\n')\n",
    "    f.write('-----\\n')\n",
    "    f.write(f'window_size:{window_size}\\n')\n",
    "    f.write(f'window_step:{window_step}\\n')\n",
    "    f.write(f'num_stations:{num_stations}\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "    suffixes = []\n",
    "    for file_path in files_found:\n",
    "        suffix = '_' + ''.join(file_path[file_path.find('_')+1:-4])\n",
    "        suffixes.append(suffix)\n",
    "\n",
    "    K = len(suffixes)\n",
    "    for k in range(K):\n",
    "\n",
    "        # ==== k fold validation ====\n",
    "        suffix = suffixes[k]\n",
    "        X_val = np.load(f\"training/X-size{window_size}-step{window_step}-station{num_stations}{suffix}.npy\")\n",
    "        Y_val = np.load(f\"training/Y-size{window_size}-step{window_step}-station{num_stations}{suffix}.npy\")\n",
    "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "        Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "        # ==== k fold training ====\n",
    "        suffix_not = [suffixes[_] for _ in range(K) if _ == k]\n",
    "        suffix_n = suffix_not[0]\n",
    "        X_train = np.load(f\"training/X-size{window_size}-step{window_step}-station{num_stations}{suffix_n}.npy\")\n",
    "        Y_train = np.load(f\"training/Y-size{window_size}-step{window_step}-station{num_stations}{suffix_n}.npy\")\n",
    "        for suffix_n in suffix_not:\n",
    "            X_next = np.load(f\"training/X-size{window_size}-step{window_step}-station{num_stations}{suffix_n}.npy\")\n",
    "            Y_next = np.load(f\"training/Y-size{window_size}-step{window_step}-station{num_stations}{suffix_n}.npy\")\n",
    "            X_train = np.vstack((X_train,X_next))\n",
    "            Y_train = np.concatenate((Y_train,Y_next))\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "        # ==== Wrap into a dataset and dataloader ====\n",
    "        dataset_train = TensorDataset(X_train, Y_train)\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True) # must shuffle\n",
    "        dataset_val = TensorDataset(X_val, Y_val)\n",
    "        dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # ==== Determine model dimensions from data ====\n",
    "        input_size = X_next.shape[2]\n",
    "        output_size = Y_next.shape[1] if Y_next.ndim == 2 else Y_next.shape[2]\n",
    "\n",
    "        # ==== Initialize Model, Loss Function, Optimizer ====\n",
    "        model = CustomLSTM(input_size, \n",
    "                        lstm_hidden_size=lstm_hidden_size,\n",
    "                        linear_hidden_size=linear_hidden_size,\n",
    "                        lstm_num_layers=lstm_num_layers,\n",
    "                        lstm_dropout=lstm_dropout,\n",
    "                        linear_dropout=linear_dropout,\n",
    "                        if_layer_norm=if_layer_norm,\n",
    "                        if_gru=if_gru,\n",
    "                        )\n",
    "        model = model.to(device)\n",
    "        criterion = ExlossUnivariate\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if_initialize = True\n",
    "        if if_initialize:\n",
    "            model.apply(init_lstm_weights)\n",
    "\n",
    "        f.write(f'KFold:{suffix}\\n')\n",
    "        f.write('\\n')\n",
    "        print(f'KFold:{suffix}\\n')\n",
    "        print()\n",
    "\n",
    "        # ==== Training Loop ====\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            ### fitting ###\n",
    "\n",
    "            # training\n",
    "            model.train()\n",
    "            for xb, yb in dataloader_train:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb,\n",
    "                                up_th,\n",
    "                                down_th,\n",
    "                                lambda_underestimate,\n",
    "                                lambda_overestimate,\n",
    "                                lambda_init\n",
    "                                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ### evaluating ###\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            running_loss_val = 0.0\n",
    "            for xb, yb in dataloader_val:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                running_loss_val += loss.item() * xb.size(0)\n",
    "            epoch_loss_val = running_loss_val / len(dataset_val)\n",
    "\n",
    "            # training\n",
    "            model.eval()\n",
    "            running_loss_train = 0.0\n",
    "            for xb, yb in dataloader_train:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                running_loss_train += loss.item() * xb.size(0)\n",
    "            epoch_loss_train = running_loss_train / len(dataset_train)\n",
    "\n",
    "            ### compute kge ###\n",
    "            model.eval()\n",
    "            # put on gpu\n",
    "            Xg = X_val.to(device)\n",
    "            Yg = Y_val.to(device)\n",
    "            Yhat = model(Xg)\n",
    "            Yg = Yg.detach().cpu()\n",
    "            Yhat = Yhat.detach().cpu()\n",
    "            Yg = Yg.numpy()\n",
    "            Yhat = Yhat.numpy()\n",
    "            Yg_reshaped = Yg.reshape(-1,1)\n",
    "            Yhat_reshaped = Yhat.reshape(-1,1)\n",
    "            Yg_unscaled = Y_loaded_scaler.inverse_transform(Yg_reshaped)\n",
    "            Yhat_unscaled = Y_loaded_scaler.inverse_transform(Yhat_reshaped)\n",
    "            Yg_undid = transform_func(Yg_unscaled)\n",
    "            Yhat_undid = transform_func(Yhat_unscaled)\n",
    "            kappa1, beta1, alpha1, R1 = kge(Yg_unscaled, Yhat_unscaled)\n",
    "            kappa2, beta2, alpha2, R2 = kge(Yg_undid, Yhat_undid)\n",
    "\n",
    "            # Print losses over time\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, TrainLoss:{epoch_loss_train:.4f}, ValLoss:{epoch_loss_val:.4f}, ValKGE(orig):{kappa2:.4f}\")\n",
    "            f.write(f\"Epoch{epoch + 1}/{num_epochs},TrainLoss:{epoch_loss_train:.4f},ValLoss:{epoch_loss_val:.4f},ValKGE(orig):{kappa2:.4f},ValBeta(orig):{beta2:.4f},ValAlpha(orig):{alpha2:.4f},ValPearson(orig):{R2:.4f}\\n\")\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoodriver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
